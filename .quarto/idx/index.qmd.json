{"title":"<p style=\"color:black,text-align:center\">Multivariate Analysis</p>","markdown":{"yaml":{"title":"<p style=\"color:black,text-align:center\">Multivariate Analysis</p>","author":[{"name":"<font color=#ff6600><b>Biometrics Unit</b></font>","affiliation":"<font color=#ff6600><b>International Institute of Tropical Agriculture (IITA)</b></font>"}]},"headingText":"[**Introduction**]{style=\"color: #234F1E;\"}","containsRefs":false,"markdown":"\n\n\nData analysis is a vital part of every research work. To achieve this, there are three types of analysis that can be appliedto datasets, these includes:\n\n1.  Univariate analysis: This involves only **one** variable (methods includes mode, mean, median, etc.).\n2.  Bivariate analysis: This involves **two** variables (method includes linear regression, correlation etc. )\n3.  Multivariate analysis: This involves **two or more** variables ( Cluster analysis, Principal Components or Factor analysis, neural network Bayesian classifier, matrix plot etc.). In multivariate analysis we are concerned with sets of objects on each of which p variables (or variates) are measured, but usually with no prior differentiation of variables into causes and effects. We can look at any one variable in isolation but to get the whole picture the variables must be considered jointly.\n\nMultivariate analysis(MVA) refers to a broad set of statistical methods designed for examining relationships among multiple variables concurrently, typically beyond two. Its purpose is to uncover intricate patterns and correlations within a dataset, offering a richer and more refined comprehension of the underlying scenario compared to simpler analyses. Through simultaneous examination of multiple variables, MVA yields deeper insights and more precise predictions, thereby bolstering decision-making within data-driven industries.\n\nIn fields like plant breeding, where understanding complex interconnections among data is crucial, this technique plays a foundation role. Variables such as days to flowering, days to maturity, 100_seed_weights and plant heights collectively influence the yield of crops. This necessitates the need for multivariate analysis.\n\nThere are several methods of multivariate analysis, but in this study, we will be considering three major methods:\n\n-   Clustering\n-   K-means clustering, and\n-   Principal Component Analysis (PCA).\n\n# [**Clustering**]{style=\"color: #234F1E;\"}\n\nA cluster analysis groups observations or variables based on similarities between them. When organizing data into clusters, the objective is for variables within the same cluster to exhibit greater similarity to each other compared to variables in different clusters. This comparison is quantified through intracluster and intercluster distances. Intracluster distance assesses the proximity of data points within a single cluster, ideally aiming for a minimal value. Intercluster distance, on the other hand, measures the separation between data points belonging to distinct clusters, with the goal of maximizing this distance for effective clustering. Data clustering techniques are valuable tools for researchers working with large databases of multivariate data.\n\nClustering analysis aims to uncover patterns within data and organize them into distinct groups based on those patterns. Hence, if two data points share similar traits, it indicates they follow the same pattern and thus should be grouped together. Through clustering analysis, we can explore which features tend to co-occur and understand the defining characteristics of each group.\n\nWe classify or cluster:\n\n(a) For data description - to understand the data better than producing a distribution.\n(b) For data reduction – i.e. reduce the size of data for, say, geographical reasons, etc.\n(c) For typology/taxonomy - to look for some natural grouping.\n\n**The major type of clustering includes:**\n\n1.  Partitioning/Centroid based clustering\n2.  Hierarchical clustering\n3.  Grid-Based clustering\n4.  Density-Based clustering\n5.  Model-Based clustering\n\n## [1. Partitioning/Centroid based clustering]{style=\"color: #002D62;\"}\n\nCentroid-based clustering is a clustering approach that divides a dataset into comparable groups by assessing the distances between their centroids. Example, k-means clustering.\n\n![](images/centroid-based.png){width=\"65%\"}\n\n## [2. Hierarchical(Agglomerative) clustering]{style=\"color: #002D62;\"}\n\nThis method identifies clusters by evaluating the proximity of data points to each other, operating on the premise that objects in closer proximity are more closely associated than those situated farther apart. To apply connectivity-based clustering, you must first select the relevant data points and quantify their similarities or differences using a distance metric. Next, a connectivity measure, like a graph or network, is created to depict the connections among the data points. Subsequently, the clustering algorithm utilizes this connectivity data to organize the data points into clusters that capture their inherent similarities. This process is often represented visually through a dendrogram, resembling a hierarchical tree structure.\n\nThe Agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It’s also known as AGNES (Agglomerative Nesting). The algorithm starts by treating each object as a singleton cluster. Next, pairs of clusters are successively merged until all clusters have been merged into one big cluster containing all objects. The result is a tree-based representation of the objects, named dendrogram.\n\nA **dendrogram** shows the hierarchical relationship between the clusters\n\n![](images/dendo.png){width=\"65%\"}\n\nThere are many distance metrics and the choice depends on the type of data:\n\n-   If the data are continuous quantitative, we can use the Euclidean distance or that of Manhattan,\n-   If the data is binary (categorical), we can use the Jaccard distance\n-   Other distance measurements include Minkowski, Canberra, etc.\n\nWhere there is no theoretical justification for an alternative, the **Euclidean distance** should generally be preferred. After the distance metric, we then have to choose a linkage criteria. There are many options: single-linkage, complete-linkage, mean or average-linkage, Ward's method, etc. Each of the methods will produce a different dendrogram. In practice, we will most often prefer the **Ward method**. Ward's method seeks to minimize intra-class variability and to maximize inter-class variability to obtain the most homogeneous possible clusters\n\n**Important things to note when working with Hierarchical clustering:**\n\n1.  **Standardize the data:** The variables to be used for clustering are of different units. Standardizing the data (mean zero, unit variance) will ensure that the data is at the same scale. We subtract each data from its mean and divide it by the standard deviation. We can use the `scale ()` function in R\n\n2.  **Dealing with missing values:** Several ways to deal with these values,\n\n    -   delete them\n    -   impute them with a mean, median, mode or use advanced regression techniques.\n\nThe root of the dendrogram corresponds to the cluster with all the individuals together. This dendrogram represents a hierarchy of partitions. The \"fusion\" distance (**height**) is indicated on the `y-axis` of the dendrogram\n\n## [3. Grid-Based clustering]{style=\"color: #002D62;\"}\n\nGrid-based clustering divides a high-dimensional dataset into cells, which are distinct sets of non-overlapping sub-regions. Every cell is given a unique identifier known as a cell ID, and all data points contained within a cell are regarded as belonging to the same cluster. This clustering technique proves efficient for analyzing expansive multidimensional datasets, as it diminishes the search time required for locating nearest neighbors, a frequent procedure in numerous clustering methodologies.\n\n![](images/grid-based.png){width=\"65%\"}\n\n## [4. Density-Based clustering]{style=\"color: #002D62;\"}\n\nDensity-based clustering represents a robust unsupervised machine learning method that enables the identification of dense clusters within a dataset. In contrast to clustering algorithms like K-means and hierarchical clustering, density-based clustering can unveil clusters of varied shapes, sizes, and densities. This technique proves particularly advantageous when handling datasets containing noise or outliers, or when the precise number of clusters within the data is unknown.\n\n![](images/density.png){width=\"65%\"}\n\n## [5. Model-Based clustering]{style=\"color: #002D62;\"}\n\nModel-based clustering organizes data points into groups according to their probability distribution, leveraging statistical patterns to delineate clusters within the dataset. Unlike centroid-based clustering, it employs statistical patterns for cluster identification. However, model-based clustering is susceptible to overfitting, wherein clustering excessively relies on the dataset, leading to inaccurate predictions.\n\n![](images/model.png){width=\"65%\"}\n\n## [Example]{style=\"color: #002D62;\"}\n\nLet's Consider the `steptoe.morex.pheno.csv` data from the `agridat` package, multi-environment trial of barley. The dataset contains 2432 observations on the following 10 variables:\n\n-   *gen* (genotype) - a Character vector for the genes of the barley\n-   *env* (environment)- a character vector of the environment\n-   *amylase* (alpha amylase), 20 Deg Units - a numeric vector for the alpha amylase sample\n-   *diapow* (diastatic power), degree units - a numeric vector for the diastatic power\n-   *hddate* (heading date),julian days - a numeric vector for the heading date\n-   *lodging*, percent- a numeric vector for the lodging\n-   *malt* (malt extract), percent - numeric vector for the malt sample\n-   *height* (plant height), centimeters - numeric vector for the height sample\n-   *protein* (grain protein), percent - numeric vector for the protein sample\n-   *yield* (grain yield), mt / ha - numeric vector for the plant yield\n\nLets take a look at a sample of the data\n\n```{r }\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\n#|message =FALSE\nlibrary(tidyverse) \ndat <- read_csv(\"steptoe.morex.pheno.csv\") \ndat\n```\n\n```{r}\nstr(dat)\n```\n\nThis shows us the structure and different data types in our dataset\n\nLet's look at the summary of the data\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nsummary(dat)\n```\n\nWe have data from 17 environments\n\n```{r }\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlevels(dat$env)\n```\n\nLet's consider only the env **ID91**\n\n```{r }\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91 <- dat %>%   filter(env==\"ID91\")\n```\n\nWe take a look at the summary of the data\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nsummary(dat.ID91)\n```\n\n`lodging` is missing for all genotypes in this environment. So we exclude `lodging` in the dataset\n\n```{r }\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91 <- dat.ID91 %>%   \n  select(-lodging) \n\nnames(dat.ID91)\n```\n\nNo more missing data, but it's a good practice to use the function `na.omit()` and exclude any missing data the dataset may have\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91 <- na.omit(dat.ID91)\n```\n\nLet's store the names of the genotypes in a vector `dat.ID91.label`\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91.label <- dat.ID91$gen\n```\n\nWe delete the gen and env columns from the dataset to only keep the quantitative variables\n\n```{r }\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91$gen <- NULL \ndat.ID91$env <- NULL \ndat.ID91\n```\n\nThe data have different units. We standardize them using the function `scale()`\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91.sc <- scale(dat.ID91) \nsummary(dat.ID91.sc)\n```\n\nThe distance metric is calculated using the Euclidean distance\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndist.ID91 <- dist(dat.ID91.sc, method = 'euclidean')\n```\n\nWe perform the hierarchical classification with the `hclust ()` function and specify the method\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='large'\nhclust.ID91 <- hclust(dist.ID91, method = 'ward.D2')\n```\n\nWe can visualize the dendrogram using the `plot()` function\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='large'\nplot(hclust.ID91, hang = -1,rect= TRUE, cex = 0.5)   \n```\n\nWe can visualize the three clusters with different colors by using the `color_branches ()` function of the `dendextend` package\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(dendextend) \ndend.ID91 <- as.dendrogram(hclust.ID91) \ncol.dend.ID91 <- color_branches(dend.ID91, k = 3)\n```\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nplot(col.dend.ID91, hang = 1, cex = 0.4)  \n```\n\nWe can get the groups to which the genotypes belong by specifying the number of clusters\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ncut.ID91 <- cutree(hclust.ID91, k = 3)\n```\n\nWe add the group names of the genotypes to the original data\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91 <- dat.ID91 %>%  \n  mutate(cluster = cut.ID91)\n```\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\n\ndat.ID91\n```\n\nWe can aggregate the data based on groups\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91.summary <- dat.ID91 %>%  \n  group_by(cluster) %>%   summarize(     amylase=mean(amylase, na.rm=TRUE),     diapow=mean(diapow, na.rm=TRUE),     hddate=mean(hddate, na.rm=TRUE),     malt=mean(malt, na.rm=TRUE),     height=mean(height, na.rm=TRUE),     protein=mean(protein, na.rm=TRUE),     yield=mean(yield, na.rm=TRUE),     Nobs=n()   ) \n\ndat.ID91.summary\n```\n\nThe table above shows the summary of mean of each variable based on the cluster. This example illustrates how clustering into groups can be done in R.\n\n# [**K-Means Clustering**]{style=\"color: #234F1E;\"}\n\nData clustering techniques serve as descriptive tools for analyzing multivariate datasets and revealing underlying structures within the data. They prove particularly beneficial when traditional second-order statistical methods, such as sample mean and covariance, are not applicable. In exploratory data analysis, where prior knowledge about the dataset's distribution is lacking, clustering becomes invaluable. It operates as a form of unsupervised classification, forming clusters by assessing similarities and dissimilarities among intrinsic characteristics of different cases. Groupings are based solely on these emergent similarities, independent of any external criteria.\n\nK-Means clustering offers a solution for analyzing unlabeled data, where clear categories or groups are absent. This algorithm provides a straightforward approach to organizing a dataset into a specified number of clusters. It is a simple approach for partitioning a dataset into K distinct, non-overlapping clusters. Through iterative steps, it assigns each data point to one of K groups, relying on feature similarities to cluster the data effectively. The K-Means technique stands out as the most straightforward and widely used clustering approach. It possesses the capability to efficiently group extensive datasets, accomplishing this task with rapid and effective computational speed.\n\nK-means clustering follows similar intuition to hierarchical clustering algorithms used for partitioning the multivariate data set. However, the number of `k` groups is pre-defined before analysis, hence the name k-means. It allows us to classify observations in a data set in multiple groups called clusters such that observations in the same cluster are as similar as possible `(i.e. high intra-class similarity)` and observations from other clusters are as dissimilar as possible `(i.e. low inter-class similarity)`. Each cluster is represented by its center (i.e, centroid) corresponding to the mean of the points assigned to the cluster.\n\n## [K-Means Clustering Steps]{style=\"color: #002D62;\"}\n\nConceptually, K-means operates in the following manner:\n\n1.  It randomly selects K centroids.\n2.  Each data point (e.g., each yield) is assigned to the closest centroid in an n-dimensional space, where n represents the number of features used in clustering (e.g., features such as water, protein, oil, and height). This allocation results in each point belonging to a specific group.\n3.  The centroids are recalculated as the mean point (vector) of all other points within the group.\n4.  Steps 2 and 3 are iteratively repeated until either the groups stabilize, meaning no points are reassigned to another centroid, or the maximum number of iterations is reached (commonly set to 10 in default settings).\n\nAs you increase the chosen value of K in clustering, the variance within the groups decreases. When K equals the number of observations, each point becomes its own group, resulting in a variance of 0.\n\nThe function `kmeans()` in the base R stats package can be used to perform K-means clustering. It is often recommended to use the `set.seed()` function to set a seed for `R’s` random number to make the results reproducible.\n\nThe basic argument of the function is as follows:\n\n-   `kmeans(df, k, nstart = x)`\n\n    -   `df` is the numeric data frame\n    -   `k` is the number of clusters\n    -   `nstart` =`x` is the number of random starting partitions, where `x>1` is recommended to have a stable result However, a beautiful cluster visualization can be created from the clusters generated with `kmeans()` function with `fviz_cluster()` function in the `factoextra` package which can be installed using `install.packages(\"factoextra\")`.\n\nThe basic argument needed in the function are:\n\n-   `fviz_cluster(km, df)`\n\n    -   `km` is the kmeans object results\n    -   `df` is the original data sets\n\n## [Example]{style=\"color: #002D62;\"}\n\nTo demonstrate how these functions work, let’s consider `steptoe.morex.pheno.csv` dataset, a multi-environment trial of barley using one environment ,`ID91`.\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(tidyverse) \ndat <- read_csv(\"steptoe.morex.pheno.csv\") \ndat\n```\n\nFilter the data set to include only the quantitative variables and the genotype name.\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91 <- dat %>%             \t\t\n  filter(env==\"ID91\") %>%             \t\t\n  select(-c(env, lodging)) %>%             \t\t\n  column_to_rownames(var = \"gen\")\nhead(dat.ID91, 4)\n```\n\n**To Scale and compute the Euclidean distance**\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91.sc <- dist(scale(dat.ID91), method = \"euclidean\")\n```\n\nThis has scaled the dataset using the euclidean distance\n\n**To Compute K-means** `(Assuming 3 clusters)`\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nset.seed(234) \nk.means.ID91 <- kmeans(dat.ID91.sc, 5, nstart=30) \n```\n\n**To Compute Cluster number for each of the observations**\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\n#|include = TRUE\nhead(k.means.ID91$cluster)\n```\n\nThis shows the cluster number for each observations.\n\n**To Compute Cluster size**\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nk.means.ID91$size\n```\n\n**To Compute Cluster means**\n\n```{r}\n#|results = 'hide'\n\nk.means.ID91$centers \n```\n\n```{r}\n#|results = 'hide\nset.seed(123) \nclustering <- kmeans(dat.ID91.sc, centers = 2, nstart = 10)  \n```\n\nUsing 3 group ( k = 3) we had 41.4% of well-grouped data.\n\nLets increase the value of k to 4 and compare the results.\n\n```{r}\n#|results = 'hide'\nset.seed(123) \nclustering <- kmeans(dat.ID91.sc, centers = 4, nstart = 10) \nclustering  \n```\n\nWith an increase in groups (k = 4), the value of well grouped data increases to 48.4%, which is a better value than the previous. This indicates that the bigger the value of K, the better the grouping gets.\n\n## [K-Means Clustering Visualization]{style=\"color: #002D62;\"}\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(factoextra) \nfviz_cluster(k.means.ID91, data = dat.ID91.sc,              ellipse.type = \"euclid\", repel = TRUE,              labelsize = 8, star.plot=TRUE)\n```\n\nThe objective of clustering analysis is to uncover patterns within the dataset. As illustrated in the plot, observations within a group typically share similar characteristics.\n\n# [**Principal Component Analysis (PCA)**]{style=\"color: #234F1E;\"}\n\nPrincipal Component Analysis (PCA), is a statistical technique aimed at revealing the covariance structure within a set of variables. It facilitates the identification of the primary directions along which the data exhibits variation.\n\nPrincipal component analysis (PCA) is a useful technique for exploratory data analysis, allowing to better visualize the variation present in a data set with many variables. It is particularly useful in the case of large data sets, where there are many variables; difficult to represent all the data in their raw format, which makes it difficult to understand the trends present inside. PCA allows us to see the general `shape` of a data, identifying which observations are similar. This can allow us to identify groups of similar observations and determine which variables make one group different from another\n\nThe basics of PCA:\n\n-   take a dataset with many variables\n-   reduce the number of variables to a smaller number of `principal components`\n-   visualize the correlations between the variables\n-   visualize the similarities between observations (individuals)\n\nThe principal components are the `directions` where there is the most variance, the directions where the data is the most distributed. This means that we are trying to find the straight line that best distributes the data as it is projected along it. The first main component is the straight line which shows the largest variation in the data\n\nPCA is a type of linear transformation that adapts a dataset to a new coordinate system such that the most significant variance is on the first coordinate, and each subsequent coordinate is orthogonal to the last and has a variance lesser. In this way, we transform a set of `x` correlated variables on `y` observations into a set of `p` uncorrelated principal components on the same observations. When many variables are correlated, they will all contribute strongly to the same principal component. Each principal component summarizes a certain percentage of the total variation in the data set. When the initial variables are highly correlated with each other, you will be able to approach most of the complexity of your dataset with just a few main components.\n\nPrincipal Component Analysis (PCA) is a widely used statistical technique for dimensionality reduction and data visualization. It aims to simplify complex datasets by transforming them into a new coordinate system, where the variables are uncorrelated, and the first few dimensions (principal components) capture the maximum variance in the data.\n\n## [Here's how PCA typically works]{style=\"color: #002D62;\"}\n\n1.  **Standardization**: If the variables in the dataset are measured on different scales, PCA often starts with standardizing the data to have a mean of 0 and a standard deviation of 1.\n\n2.  **Covariance Matrix**: PCA calculates the covariance matrix of the standardized data. This matrix summarizes the relationships between pairs of variables, indicating how much they vary together.\n\n3.  **Eigenvalue Decomposition**: PCA then decomposes the covariance matrix into its eigenvectors and eigenvalues. The eigenvectors represent the directions of maximum variance in the data, and the corresponding eigenvalues indicate the magnitude of variance along those directions.\n\n4.  **Selecting Principal Components**: The eigenvectors are ranked based on their corresponding eigenvalues, with the first eigenvector capturing the most variance, the second capturing the second most variance, and so on. Typically, only a subset of the top-ranked eigenvectors, called principal components, are retained, thereby reducing the dimensionality of the dataset.\n\n5.  **Dimensionality Reduction**: By projecting the original data onto the selected principal components, PCA creates a lower-dimensional representation of the dataset while preserving as much of the variance as possible. This reduced representation can often capture the essential patterns and structures in the data.\n\n## [Application of PCA]{style=\"color: #002D62;\"}\n\n-   **Dimensionality Reduction**: PCA can reduce the number of variables in a dataset while retaining most of the variance, making it easier to analyze and visualize high-dimensional data.\n\n-   **Data Visualization**: PCA can be used to visualize high-dimensional data in lower dimensions (e.g., 2D or 3D), allowing for easier interpretation and exploration of data patterns.\n\n-   **Feature Extraction**: PCA can extract new features (principal components) that are linear combinations of the original variables. These new features may capture important underlying patterns in the data.\n\nOverall, PCA is a versatile tool widely used in various fields such as statistics, machine learning, finance, image processing, and genetics for data preprocessing, visualization, and feature extraction.\n\n## [Example]{style=\"color: #002D62;\"}\n\nLet's consider the `australia.soybean.csv`, a multi-environment trial of 58 varieties of soybeans, in 4 locations across 2 years in Australia. The description of the data are:\n\n-   **env** (environment), 8 levels,\n-   **loc** (location)\n-   **year**\n-   **gen** (genotype) of soybeans,\n-   **yield**, metric tons/hectare\n-   **height**, in meters\n-   **lodging**, number of plants\n-   **size seed**, in millimetres\n-   **protein**, percentage\n-   **oil**, percentage\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(tidyverse) \ndat <- read_csv(\"australia.soybean.csv\")\n```\n\nTo study the relationship between two variables, say `yield` and `oil`, we can use + Pearson correlation coefficient + Simple linear regression\n\n```{r}\n#|#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\n#|echo = FALSE\n\nggplot(dat) +   \n  aes(x = oil, y = yield) +   \n  geom_point() +   \n  geom_smooth(method = lm)\n```\n\n![](images/pca-oil-yield.png) The diagram above indicates a positive linear relationship between yield and oil content of soybeans.\n\nLets consider one location `Brookstead` in the `australia.soybean` data\n\n```{r eval=TRUE, message = FALSE, tidy=FALSE, size='tiny'}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\n#|message = FALSE\ndat.Brookstead <- dat %>%   \n  filter(loc==\"Brookstead\") \nstr(dat.Brookstead)\n```\n\nWe store the names of the genotypes in a vector\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlabels.gen <- dat.Brookstead$gen\n```\n\nAnd only consider the quantitative variables\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.Brookstead <- dat.Brookstead %>%  \n  select(-c(env, loc, year, gen))\n```\n\nWe run the PCA using the prcomp function\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.Brookstead.pca<-prcomp(dat.Brookstead, center=TRUE, scale=TRUE) \nsummary(dat.Brookstead.pca)\n```\n\nThe first 2 axes explain more than 75% of the variation in the dataset. We visualize the PCA using a `biplot`. A `biplot` is a type of graph that allows us to simultaneously visualize the observations and the variables. We use the `ggbiplot` package to visualize the biplot.\n\n```{r}\n#|eval=FALSE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(ggbiplot) \nggbiplot(dat.Brookstead.pca) +   \n  labs(title = \"Yield and other traits of soybeans in Australia\",        subtitle = \"Period 1970-1971\",        caption = \"Data: agridat::australia.soybean\"   ) +  \n  theme_classic()\n```\n\n![](images/pca-biplot-1.png){width=\"65%\"}\n\nThe components are seen as arrows coming from the origin. Here, all the variables contribute to PC1, with:\n\n-   higher values of `oil`, `size` and `yield` to the right on this graph and lower values to the left\n-   vice versa for `protein`, `lodging` and `height`\n\nThis allows us to see how the observations relate to the components. However, this is not very informative if we do not identify the observations. We can display the names of the genotypes.\n\n```{r}\n#|eval=FALSE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(ggbiplot) \nggbiplot(dat.Brookstead.pca, labels=labels.gen) + #<<\n  labs(title = \"Yield and other traits of soybeans in Australia\",        subtitle = \"Period 1970-1971\",        caption = \"Data: agridat::australia.soybean\"   ) +   \n  theme_classic()\n```\n\n![](images/pca-biplot-2.png){width=\"65%\"}\n\nThe diagram shows in detail the observations per each genotype.\n\nWe have two years of testing, 1970 and 1971. Could be interesting to identify the genotypes by year\n\n```{r}\n#|eval=FALSE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(ggbiplot) \n#ggbiplot(dat.Brookstead.pca, labels=labels.gen, groups=labels.year)+ #<<   \n  #labs(title = \"Yield and other traits of soybeans in Australia\",        subtitle = \"Period 1970-1971\",        caption = \"Data: agridat::australia.soybean\"   ) +\n  #theme_classic()\n```\n\n![](images/pca-biplot-3.png){width=\"65%\"}\n","srcMarkdownNoYaml":"\n\n# [**Introduction**]{style=\"color: #234F1E;\"}\n\nData analysis is a vital part of every research work. To achieve this, there are three types of analysis that can be appliedto datasets, these includes:\n\n1.  Univariate analysis: This involves only **one** variable (methods includes mode, mean, median, etc.).\n2.  Bivariate analysis: This involves **two** variables (method includes linear regression, correlation etc. )\n3.  Multivariate analysis: This involves **two or more** variables ( Cluster analysis, Principal Components or Factor analysis, neural network Bayesian classifier, matrix plot etc.). In multivariate analysis we are concerned with sets of objects on each of which p variables (or variates) are measured, but usually with no prior differentiation of variables into causes and effects. We can look at any one variable in isolation but to get the whole picture the variables must be considered jointly.\n\nMultivariate analysis(MVA) refers to a broad set of statistical methods designed for examining relationships among multiple variables concurrently, typically beyond two. Its purpose is to uncover intricate patterns and correlations within a dataset, offering a richer and more refined comprehension of the underlying scenario compared to simpler analyses. Through simultaneous examination of multiple variables, MVA yields deeper insights and more precise predictions, thereby bolstering decision-making within data-driven industries.\n\nIn fields like plant breeding, where understanding complex interconnections among data is crucial, this technique plays a foundation role. Variables such as days to flowering, days to maturity, 100_seed_weights and plant heights collectively influence the yield of crops. This necessitates the need for multivariate analysis.\n\nThere are several methods of multivariate analysis, but in this study, we will be considering three major methods:\n\n-   Clustering\n-   K-means clustering, and\n-   Principal Component Analysis (PCA).\n\n# [**Clustering**]{style=\"color: #234F1E;\"}\n\nA cluster analysis groups observations or variables based on similarities between them. When organizing data into clusters, the objective is for variables within the same cluster to exhibit greater similarity to each other compared to variables in different clusters. This comparison is quantified through intracluster and intercluster distances. Intracluster distance assesses the proximity of data points within a single cluster, ideally aiming for a minimal value. Intercluster distance, on the other hand, measures the separation between data points belonging to distinct clusters, with the goal of maximizing this distance for effective clustering. Data clustering techniques are valuable tools for researchers working with large databases of multivariate data.\n\nClustering analysis aims to uncover patterns within data and organize them into distinct groups based on those patterns. Hence, if two data points share similar traits, it indicates they follow the same pattern and thus should be grouped together. Through clustering analysis, we can explore which features tend to co-occur and understand the defining characteristics of each group.\n\nWe classify or cluster:\n\n(a) For data description - to understand the data better than producing a distribution.\n(b) For data reduction – i.e. reduce the size of data for, say, geographical reasons, etc.\n(c) For typology/taxonomy - to look for some natural grouping.\n\n**The major type of clustering includes:**\n\n1.  Partitioning/Centroid based clustering\n2.  Hierarchical clustering\n3.  Grid-Based clustering\n4.  Density-Based clustering\n5.  Model-Based clustering\n\n## [1. Partitioning/Centroid based clustering]{style=\"color: #002D62;\"}\n\nCentroid-based clustering is a clustering approach that divides a dataset into comparable groups by assessing the distances between their centroids. Example, k-means clustering.\n\n![](images/centroid-based.png){width=\"65%\"}\n\n## [2. Hierarchical(Agglomerative) clustering]{style=\"color: #002D62;\"}\n\nThis method identifies clusters by evaluating the proximity of data points to each other, operating on the premise that objects in closer proximity are more closely associated than those situated farther apart. To apply connectivity-based clustering, you must first select the relevant data points and quantify their similarities or differences using a distance metric. Next, a connectivity measure, like a graph or network, is created to depict the connections among the data points. Subsequently, the clustering algorithm utilizes this connectivity data to organize the data points into clusters that capture their inherent similarities. This process is often represented visually through a dendrogram, resembling a hierarchical tree structure.\n\nThe Agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It’s also known as AGNES (Agglomerative Nesting). The algorithm starts by treating each object as a singleton cluster. Next, pairs of clusters are successively merged until all clusters have been merged into one big cluster containing all objects. The result is a tree-based representation of the objects, named dendrogram.\n\nA **dendrogram** shows the hierarchical relationship between the clusters\n\n![](images/dendo.png){width=\"65%\"}\n\nThere are many distance metrics and the choice depends on the type of data:\n\n-   If the data are continuous quantitative, we can use the Euclidean distance or that of Manhattan,\n-   If the data is binary (categorical), we can use the Jaccard distance\n-   Other distance measurements include Minkowski, Canberra, etc.\n\nWhere there is no theoretical justification for an alternative, the **Euclidean distance** should generally be preferred. After the distance metric, we then have to choose a linkage criteria. There are many options: single-linkage, complete-linkage, mean or average-linkage, Ward's method, etc. Each of the methods will produce a different dendrogram. In practice, we will most often prefer the **Ward method**. Ward's method seeks to minimize intra-class variability and to maximize inter-class variability to obtain the most homogeneous possible clusters\n\n**Important things to note when working with Hierarchical clustering:**\n\n1.  **Standardize the data:** The variables to be used for clustering are of different units. Standardizing the data (mean zero, unit variance) will ensure that the data is at the same scale. We subtract each data from its mean and divide it by the standard deviation. We can use the `scale ()` function in R\n\n2.  **Dealing with missing values:** Several ways to deal with these values,\n\n    -   delete them\n    -   impute them with a mean, median, mode or use advanced regression techniques.\n\nThe root of the dendrogram corresponds to the cluster with all the individuals together. This dendrogram represents a hierarchy of partitions. The \"fusion\" distance (**height**) is indicated on the `y-axis` of the dendrogram\n\n## [3. Grid-Based clustering]{style=\"color: #002D62;\"}\n\nGrid-based clustering divides a high-dimensional dataset into cells, which are distinct sets of non-overlapping sub-regions. Every cell is given a unique identifier known as a cell ID, and all data points contained within a cell are regarded as belonging to the same cluster. This clustering technique proves efficient for analyzing expansive multidimensional datasets, as it diminishes the search time required for locating nearest neighbors, a frequent procedure in numerous clustering methodologies.\n\n![](images/grid-based.png){width=\"65%\"}\n\n## [4. Density-Based clustering]{style=\"color: #002D62;\"}\n\nDensity-based clustering represents a robust unsupervised machine learning method that enables the identification of dense clusters within a dataset. In contrast to clustering algorithms like K-means and hierarchical clustering, density-based clustering can unveil clusters of varied shapes, sizes, and densities. This technique proves particularly advantageous when handling datasets containing noise or outliers, or when the precise number of clusters within the data is unknown.\n\n![](images/density.png){width=\"65%\"}\n\n## [5. Model-Based clustering]{style=\"color: #002D62;\"}\n\nModel-based clustering organizes data points into groups according to their probability distribution, leveraging statistical patterns to delineate clusters within the dataset. Unlike centroid-based clustering, it employs statistical patterns for cluster identification. However, model-based clustering is susceptible to overfitting, wherein clustering excessively relies on the dataset, leading to inaccurate predictions.\n\n![](images/model.png){width=\"65%\"}\n\n## [Example]{style=\"color: #002D62;\"}\n\nLet's Consider the `steptoe.morex.pheno.csv` data from the `agridat` package, multi-environment trial of barley. The dataset contains 2432 observations on the following 10 variables:\n\n-   *gen* (genotype) - a Character vector for the genes of the barley\n-   *env* (environment)- a character vector of the environment\n-   *amylase* (alpha amylase), 20 Deg Units - a numeric vector for the alpha amylase sample\n-   *diapow* (diastatic power), degree units - a numeric vector for the diastatic power\n-   *hddate* (heading date),julian days - a numeric vector for the heading date\n-   *lodging*, percent- a numeric vector for the lodging\n-   *malt* (malt extract), percent - numeric vector for the malt sample\n-   *height* (plant height), centimeters - numeric vector for the height sample\n-   *protein* (grain protein), percent - numeric vector for the protein sample\n-   *yield* (grain yield), mt / ha - numeric vector for the plant yield\n\nLets take a look at a sample of the data\n\n```{r }\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\n#|message =FALSE\nlibrary(tidyverse) \ndat <- read_csv(\"steptoe.morex.pheno.csv\") \ndat\n```\n\n```{r}\nstr(dat)\n```\n\nThis shows us the structure and different data types in our dataset\n\nLet's look at the summary of the data\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nsummary(dat)\n```\n\nWe have data from 17 environments\n\n```{r }\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlevels(dat$env)\n```\n\nLet's consider only the env **ID91**\n\n```{r }\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91 <- dat %>%   filter(env==\"ID91\")\n```\n\nWe take a look at the summary of the data\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nsummary(dat.ID91)\n```\n\n`lodging` is missing for all genotypes in this environment. So we exclude `lodging` in the dataset\n\n```{r }\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91 <- dat.ID91 %>%   \n  select(-lodging) \n\nnames(dat.ID91)\n```\n\nNo more missing data, but it's a good practice to use the function `na.omit()` and exclude any missing data the dataset may have\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91 <- na.omit(dat.ID91)\n```\n\nLet's store the names of the genotypes in a vector `dat.ID91.label`\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91.label <- dat.ID91$gen\n```\n\nWe delete the gen and env columns from the dataset to only keep the quantitative variables\n\n```{r }\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91$gen <- NULL \ndat.ID91$env <- NULL \ndat.ID91\n```\n\nThe data have different units. We standardize them using the function `scale()`\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91.sc <- scale(dat.ID91) \nsummary(dat.ID91.sc)\n```\n\nThe distance metric is calculated using the Euclidean distance\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndist.ID91 <- dist(dat.ID91.sc, method = 'euclidean')\n```\n\nWe perform the hierarchical classification with the `hclust ()` function and specify the method\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='large'\nhclust.ID91 <- hclust(dist.ID91, method = 'ward.D2')\n```\n\nWe can visualize the dendrogram using the `plot()` function\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='large'\nplot(hclust.ID91, hang = -1,rect= TRUE, cex = 0.5)   \n```\n\nWe can visualize the three clusters with different colors by using the `color_branches ()` function of the `dendextend` package\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(dendextend) \ndend.ID91 <- as.dendrogram(hclust.ID91) \ncol.dend.ID91 <- color_branches(dend.ID91, k = 3)\n```\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nplot(col.dend.ID91, hang = 1, cex = 0.4)  \n```\n\nWe can get the groups to which the genotypes belong by specifying the number of clusters\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ncut.ID91 <- cutree(hclust.ID91, k = 3)\n```\n\nWe add the group names of the genotypes to the original data\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91 <- dat.ID91 %>%  \n  mutate(cluster = cut.ID91)\n```\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\n\ndat.ID91\n```\n\nWe can aggregate the data based on groups\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91.summary <- dat.ID91 %>%  \n  group_by(cluster) %>%   summarize(     amylase=mean(amylase, na.rm=TRUE),     diapow=mean(diapow, na.rm=TRUE),     hddate=mean(hddate, na.rm=TRUE),     malt=mean(malt, na.rm=TRUE),     height=mean(height, na.rm=TRUE),     protein=mean(protein, na.rm=TRUE),     yield=mean(yield, na.rm=TRUE),     Nobs=n()   ) \n\ndat.ID91.summary\n```\n\nThe table above shows the summary of mean of each variable based on the cluster. This example illustrates how clustering into groups can be done in R.\n\n# [**K-Means Clustering**]{style=\"color: #234F1E;\"}\n\nData clustering techniques serve as descriptive tools for analyzing multivariate datasets and revealing underlying structures within the data. They prove particularly beneficial when traditional second-order statistical methods, such as sample mean and covariance, are not applicable. In exploratory data analysis, where prior knowledge about the dataset's distribution is lacking, clustering becomes invaluable. It operates as a form of unsupervised classification, forming clusters by assessing similarities and dissimilarities among intrinsic characteristics of different cases. Groupings are based solely on these emergent similarities, independent of any external criteria.\n\nK-Means clustering offers a solution for analyzing unlabeled data, where clear categories or groups are absent. This algorithm provides a straightforward approach to organizing a dataset into a specified number of clusters. It is a simple approach for partitioning a dataset into K distinct, non-overlapping clusters. Through iterative steps, it assigns each data point to one of K groups, relying on feature similarities to cluster the data effectively. The K-Means technique stands out as the most straightforward and widely used clustering approach. It possesses the capability to efficiently group extensive datasets, accomplishing this task with rapid and effective computational speed.\n\nK-means clustering follows similar intuition to hierarchical clustering algorithms used for partitioning the multivariate data set. However, the number of `k` groups is pre-defined before analysis, hence the name k-means. It allows us to classify observations in a data set in multiple groups called clusters such that observations in the same cluster are as similar as possible `(i.e. high intra-class similarity)` and observations from other clusters are as dissimilar as possible `(i.e. low inter-class similarity)`. Each cluster is represented by its center (i.e, centroid) corresponding to the mean of the points assigned to the cluster.\n\n## [K-Means Clustering Steps]{style=\"color: #002D62;\"}\n\nConceptually, K-means operates in the following manner:\n\n1.  It randomly selects K centroids.\n2.  Each data point (e.g., each yield) is assigned to the closest centroid in an n-dimensional space, where n represents the number of features used in clustering (e.g., features such as water, protein, oil, and height). This allocation results in each point belonging to a specific group.\n3.  The centroids are recalculated as the mean point (vector) of all other points within the group.\n4.  Steps 2 and 3 are iteratively repeated until either the groups stabilize, meaning no points are reassigned to another centroid, or the maximum number of iterations is reached (commonly set to 10 in default settings).\n\nAs you increase the chosen value of K in clustering, the variance within the groups decreases. When K equals the number of observations, each point becomes its own group, resulting in a variance of 0.\n\nThe function `kmeans()` in the base R stats package can be used to perform K-means clustering. It is often recommended to use the `set.seed()` function to set a seed for `R’s` random number to make the results reproducible.\n\nThe basic argument of the function is as follows:\n\n-   `kmeans(df, k, nstart = x)`\n\n    -   `df` is the numeric data frame\n    -   `k` is the number of clusters\n    -   `nstart` =`x` is the number of random starting partitions, where `x>1` is recommended to have a stable result However, a beautiful cluster visualization can be created from the clusters generated with `kmeans()` function with `fviz_cluster()` function in the `factoextra` package which can be installed using `install.packages(\"factoextra\")`.\n\nThe basic argument needed in the function are:\n\n-   `fviz_cluster(km, df)`\n\n    -   `km` is the kmeans object results\n    -   `df` is the original data sets\n\n## [Example]{style=\"color: #002D62;\"}\n\nTo demonstrate how these functions work, let’s consider `steptoe.morex.pheno.csv` dataset, a multi-environment trial of barley using one environment ,`ID91`.\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(tidyverse) \ndat <- read_csv(\"steptoe.morex.pheno.csv\") \ndat\n```\n\nFilter the data set to include only the quantitative variables and the genotype name.\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91 <- dat %>%             \t\t\n  filter(env==\"ID91\") %>%             \t\t\n  select(-c(env, lodging)) %>%             \t\t\n  column_to_rownames(var = \"gen\")\nhead(dat.ID91, 4)\n```\n\n**To Scale and compute the Euclidean distance**\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.ID91.sc <- dist(scale(dat.ID91), method = \"euclidean\")\n```\n\nThis has scaled the dataset using the euclidean distance\n\n**To Compute K-means** `(Assuming 3 clusters)`\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nset.seed(234) \nk.means.ID91 <- kmeans(dat.ID91.sc, 5, nstart=30) \n```\n\n**To Compute Cluster number for each of the observations**\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\n#|include = TRUE\nhead(k.means.ID91$cluster)\n```\n\nThis shows the cluster number for each observations.\n\n**To Compute Cluster size**\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nk.means.ID91$size\n```\n\n**To Compute Cluster means**\n\n```{r}\n#|results = 'hide'\n\nk.means.ID91$centers \n```\n\n```{r}\n#|results = 'hide\nset.seed(123) \nclustering <- kmeans(dat.ID91.sc, centers = 2, nstart = 10)  \n```\n\nUsing 3 group ( k = 3) we had 41.4% of well-grouped data.\n\nLets increase the value of k to 4 and compare the results.\n\n```{r}\n#|results = 'hide'\nset.seed(123) \nclustering <- kmeans(dat.ID91.sc, centers = 4, nstart = 10) \nclustering  \n```\n\nWith an increase in groups (k = 4), the value of well grouped data increases to 48.4%, which is a better value than the previous. This indicates that the bigger the value of K, the better the grouping gets.\n\n## [K-Means Clustering Visualization]{style=\"color: #002D62;\"}\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(factoextra) \nfviz_cluster(k.means.ID91, data = dat.ID91.sc,              ellipse.type = \"euclid\", repel = TRUE,              labelsize = 8, star.plot=TRUE)\n```\n\nThe objective of clustering analysis is to uncover patterns within the dataset. As illustrated in the plot, observations within a group typically share similar characteristics.\n\n# [**Principal Component Analysis (PCA)**]{style=\"color: #234F1E;\"}\n\nPrincipal Component Analysis (PCA), is a statistical technique aimed at revealing the covariance structure within a set of variables. It facilitates the identification of the primary directions along which the data exhibits variation.\n\nPrincipal component analysis (PCA) is a useful technique for exploratory data analysis, allowing to better visualize the variation present in a data set with many variables. It is particularly useful in the case of large data sets, where there are many variables; difficult to represent all the data in their raw format, which makes it difficult to understand the trends present inside. PCA allows us to see the general `shape` of a data, identifying which observations are similar. This can allow us to identify groups of similar observations and determine which variables make one group different from another\n\nThe basics of PCA:\n\n-   take a dataset with many variables\n-   reduce the number of variables to a smaller number of `principal components`\n-   visualize the correlations between the variables\n-   visualize the similarities between observations (individuals)\n\nThe principal components are the `directions` where there is the most variance, the directions where the data is the most distributed. This means that we are trying to find the straight line that best distributes the data as it is projected along it. The first main component is the straight line which shows the largest variation in the data\n\nPCA is a type of linear transformation that adapts a dataset to a new coordinate system such that the most significant variance is on the first coordinate, and each subsequent coordinate is orthogonal to the last and has a variance lesser. In this way, we transform a set of `x` correlated variables on `y` observations into a set of `p` uncorrelated principal components on the same observations. When many variables are correlated, they will all contribute strongly to the same principal component. Each principal component summarizes a certain percentage of the total variation in the data set. When the initial variables are highly correlated with each other, you will be able to approach most of the complexity of your dataset with just a few main components.\n\nPrincipal Component Analysis (PCA) is a widely used statistical technique for dimensionality reduction and data visualization. It aims to simplify complex datasets by transforming them into a new coordinate system, where the variables are uncorrelated, and the first few dimensions (principal components) capture the maximum variance in the data.\n\n## [Here's how PCA typically works]{style=\"color: #002D62;\"}\n\n1.  **Standardization**: If the variables in the dataset are measured on different scales, PCA often starts with standardizing the data to have a mean of 0 and a standard deviation of 1.\n\n2.  **Covariance Matrix**: PCA calculates the covariance matrix of the standardized data. This matrix summarizes the relationships between pairs of variables, indicating how much they vary together.\n\n3.  **Eigenvalue Decomposition**: PCA then decomposes the covariance matrix into its eigenvectors and eigenvalues. The eigenvectors represent the directions of maximum variance in the data, and the corresponding eigenvalues indicate the magnitude of variance along those directions.\n\n4.  **Selecting Principal Components**: The eigenvectors are ranked based on their corresponding eigenvalues, with the first eigenvector capturing the most variance, the second capturing the second most variance, and so on. Typically, only a subset of the top-ranked eigenvectors, called principal components, are retained, thereby reducing the dimensionality of the dataset.\n\n5.  **Dimensionality Reduction**: By projecting the original data onto the selected principal components, PCA creates a lower-dimensional representation of the dataset while preserving as much of the variance as possible. This reduced representation can often capture the essential patterns and structures in the data.\n\n## [Application of PCA]{style=\"color: #002D62;\"}\n\n-   **Dimensionality Reduction**: PCA can reduce the number of variables in a dataset while retaining most of the variance, making it easier to analyze and visualize high-dimensional data.\n\n-   **Data Visualization**: PCA can be used to visualize high-dimensional data in lower dimensions (e.g., 2D or 3D), allowing for easier interpretation and exploration of data patterns.\n\n-   **Feature Extraction**: PCA can extract new features (principal components) that are linear combinations of the original variables. These new features may capture important underlying patterns in the data.\n\nOverall, PCA is a versatile tool widely used in various fields such as statistics, machine learning, finance, image processing, and genetics for data preprocessing, visualization, and feature extraction.\n\n## [Example]{style=\"color: #002D62;\"}\n\nLet's consider the `australia.soybean.csv`, a multi-environment trial of 58 varieties of soybeans, in 4 locations across 2 years in Australia. The description of the data are:\n\n-   **env** (environment), 8 levels,\n-   **loc** (location)\n-   **year**\n-   **gen** (genotype) of soybeans,\n-   **yield**, metric tons/hectare\n-   **height**, in meters\n-   **lodging**, number of plants\n-   **size seed**, in millimetres\n-   **protein**, percentage\n-   **oil**, percentage\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(tidyverse) \ndat <- read_csv(\"australia.soybean.csv\")\n```\n\nTo study the relationship between two variables, say `yield` and `oil`, we can use + Pearson correlation coefficient + Simple linear regression\n\n```{r}\n#|#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\n#|echo = FALSE\n\nggplot(dat) +   \n  aes(x = oil, y = yield) +   \n  geom_point() +   \n  geom_smooth(method = lm)\n```\n\n![](images/pca-oil-yield.png) The diagram above indicates a positive linear relationship between yield and oil content of soybeans.\n\nLets consider one location `Brookstead` in the `australia.soybean` data\n\n```{r eval=TRUE, message = FALSE, tidy=FALSE, size='tiny'}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\n#|message = FALSE\ndat.Brookstead <- dat %>%   \n  filter(loc==\"Brookstead\") \nstr(dat.Brookstead)\n```\n\nWe store the names of the genotypes in a vector\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\nlabels.gen <- dat.Brookstead$gen\n```\n\nAnd only consider the quantitative variables\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.Brookstead <- dat.Brookstead %>%  \n  select(-c(env, loc, year, gen))\n```\n\nWe run the PCA using the prcomp function\n\n```{r}\n#|eval=TRUE\n#|tidy=FALSE\n#|size='tiny'\ndat.Brookstead.pca<-prcomp(dat.Brookstead, center=TRUE, scale=TRUE) \nsummary(dat.Brookstead.pca)\n```\n\nThe first 2 axes explain more than 75% of the variation in the dataset. We visualize the PCA using a `biplot`. A `biplot` is a type of graph that allows us to simultaneously visualize the observations and the variables. We use the `ggbiplot` package to visualize the biplot.\n\n```{r}\n#|eval=FALSE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(ggbiplot) \nggbiplot(dat.Brookstead.pca) +   \n  labs(title = \"Yield and other traits of soybeans in Australia\",        subtitle = \"Period 1970-1971\",        caption = \"Data: agridat::australia.soybean\"   ) +  \n  theme_classic()\n```\n\n![](images/pca-biplot-1.png){width=\"65%\"}\n\nThe components are seen as arrows coming from the origin. Here, all the variables contribute to PC1, with:\n\n-   higher values of `oil`, `size` and `yield` to the right on this graph and lower values to the left\n-   vice versa for `protein`, `lodging` and `height`\n\nThis allows us to see how the observations relate to the components. However, this is not very informative if we do not identify the observations. We can display the names of the genotypes.\n\n```{r}\n#|eval=FALSE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(ggbiplot) \nggbiplot(dat.Brookstead.pca, labels=labels.gen) + #<<\n  labs(title = \"Yield and other traits of soybeans in Australia\",        subtitle = \"Period 1970-1971\",        caption = \"Data: agridat::australia.soybean\"   ) +   \n  theme_classic()\n```\n\n![](images/pca-biplot-2.png){width=\"65%\"}\n\nThe diagram shows in detail the observations per each genotype.\n\nWe have two years of testing, 1970 and 1971. Could be interesting to identify the genotypes by year\n\n```{r}\n#|eval=FALSE\n#|tidy=FALSE\n#|size='tiny'\nlibrary(ggbiplot) \n#ggbiplot(dat.Brookstead.pca, labels=labels.gen, groups=labels.year)+ #<<   \n  #labs(title = \"Yield and other traits of soybeans in Australia\",        subtitle = \"Period 1970-1971\",        caption = \"Data: agridat::australia.soybean\"   ) +\n  #theme_classic()\n```\n\n![](images/pca-biplot-3.png){width=\"65%\"}\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","editor":"visual","theme":"cosmo","title":"<p style=\"color:black,text-align:center\">Multivariate Analysis</p>","author":[{"name":"<font color=#ff6600><b>Biometrics Unit</b></font>","affiliation":"<font color=#ff6600><b>International Institute of Tropical Agriculture (IITA)</b></font>"}]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}